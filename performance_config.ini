# LLM Council Performance Configuration
# Optimize settings for faster local model responses

[ollama_optimization]
# Enable speed optimizations for Ollama local models
optimize_speed = true

# Context window size (lower = faster)
# Default: 2048 (optimized), Standard: 4096
num_ctx = 2048

# Maximum tokens to generate (lower = faster)
# Default: 512 (optimized), Standard: 1024
num_predict = 512

# Temperature for response generation
# Lower = more focused, Higher = more creative
# Default: 0.7 (optimized), Standard: 0.8
temperature = 0.7

# Top-p nucleus sampling
# Default: 0.9
top_p = 0.9

# Top-k token selection
# Default: 40
top_k = 40

[debate_council_optimization]
# Use shorter, more concise prompts for local models
use_concise_prompts = true

# Skip chat history for faster processing (not recommended for conversations)
skip_history = false

# Maximum history messages to include
max_history_messages = 5

[general]
# Timeout for model responses (seconds)
response_timeout = 30

# Enable parallel processing for council members (experimental)
parallel_processing = false

# Cache model responses (saves time on repeated queries)
enable_caching = false
